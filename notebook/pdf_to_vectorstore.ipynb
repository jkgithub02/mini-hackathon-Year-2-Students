{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "config = {\n",
    "    \"api_type\": os.getenv(\"AZURE_OPENAI_API_TYPE\"),\n",
    "    \"api_base\": os.getenv(\"AZURE_OPENAI_API_BASE\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"deployment_name\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"model_name\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "AZURE_OPENAI_API_KEY = config[\"api_key\"]\n",
    "AZURE_OPENAI_ENDPOINT = config[\"api_base\"]\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = \"text-embedding-ada-002\"\n",
    "\n",
    "openai.api_type = config[\"api_type\"]\n",
    "openai.api_key = config[\"api_key\"]\n",
    "openai.api_base = config[\"api_base\"]\n",
    "openai.api_version = config[\"api_version\"]\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    openai_api_version=config[\"api_version\"],\n",
    "    deployment=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    openai_api_type=config[\"api_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdfreader = PdfReader('./pdf/99speedmart.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content\n",
    "\n",
    "raw_text = \"\\n\".join([line for line in raw_text.splitlines() if line.strip()])\n",
    "\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = [Document(page_content=raw_text)]\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Run Embeddings\n",
    "# -----------------------------\n",
    "embedding_data = []\n",
    "\n",
    "# Generate embeddings for each text chunk\n",
    "for idx, doc in enumerate(texts):\n",
    "    # Access the page_content of the Document\n",
    "    text = doc.page_content.strip()  # Assuming texts are Document objects\n",
    "    if not text:  # Skip empty chunks\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Generate the embedding for the current text chunk\n",
    "        embedding = embeddings.embed_query(text)  # Ensure this method exists\n",
    "        embedding_data.append({\n",
    "            'chunk': idx + 1,\n",
    "            'text': text,\n",
    "            'embedding': embedding\n",
    "        })\n",
    "        print(f\"Generated embedding for Chunk {idx + 1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for Chunk {idx + 1}: {e}\")\n",
    "\n",
    "# Optionally, print the resulting embedding data\n",
    "for data in embedding_data:\n",
    "    print(f\"Chunk {data['chunk']}:\")\n",
    "    print(f\"Text: {data['text']}\")\n",
    "    print(f\"Embedding: {data['embedding']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in embedding_data:\n",
    "    print(f\"Chunk {item['chunk']} Embedding:\\n{item['embedding']}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "embeddings_list = [item['embedding'] for item in embedding_data]\n",
    "embeddings_matrix = np.array(embeddings_list).astype('float32')  # Faiss requires float32\n",
    "\n",
    "# Extract metadata\n",
    "metadata = [item['text'] for item in embedding_data]\n",
    "\n",
    "# Determine the dimensionality of the embeddings\n",
    "dimension = embeddings_matrix.shape[1]\n",
    "\n",
    "# Initialize the Faiss index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Verify if GPU is available and use it\n",
    "if faiss.get_num_gpus() > 0:\n",
    "    print(\"Using GPU for Faiss index\")\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "else:\n",
    "    print(\"Using CPU for Faiss index\")\n",
    "\n",
    "\n",
    "# Add embeddings to the Faiss index\n",
    "index.add(embeddings_matrix)\n",
    "print(f\"Number of vectors in the index: {index.ntotal}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Persisting Faiss Index and Metadata\n",
    "# -----------------------------\n",
    "\n",
    "# Save the index to a file\n",
    "faiss.write_index(index, \"faiss_index.index\")\n",
    "print(\"Faiss index saved to faiss_index.index\")\n",
    "\n",
    "# Save metadata\n",
    "with open(\"metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"Metadata saved to metadata.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, pickle\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Loading Faiss for Saved Index & Metadata\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "index = faiss.read_index(\"faiss_index.index\")\n",
    "if faiss.get_num_gpus() > 0:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "with open(\"metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ask a Question\n",
    "# -----------------------------\n",
    "query = \"What is the share price in 2019?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Performing a Similarity Search\n",
    "# -----------------------------\n",
    "\n",
    "# Generate embedding for the query\n",
    "try:\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    query_embedding_np = np.array([query_embedding]).astype('float32')\n",
    "except Exception as e:\n",
    "    print(f\"Error generating embedding for query: {e}\")\n",
    "\n",
    "# Number of nearest neighbors\n",
    "k = 3\n",
    "\n",
    "# Perform the search\n",
    "distances, indices = index.search(query_embedding_np, k)\n",
    "\n",
    "# Display the results\n",
    "for i in range(k):\n",
    "    idx = indices[0][i]\n",
    "    distance = distances[0][i]\n",
    "    print(f\"Rank {i + 1}:\")\n",
    "    print(f\"Text: {metadata[idx]}\")\n",
    "    print(f\"Distance: {distance}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Answer with LLM\n",
    "# -----------------------------\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "documents = [Document(page_content=text, metadata={}) for text in metadata]\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=config[\"deployment_name\"],\n",
    "    model_name=config[\"model_name\"],\n",
    "    azure_endpoint=config[\"api_base\"],\n",
    "    openai_api_key=config[\"api_key\"],\n",
    "    openai_api_version=config[\"api_version\"],\n",
    "    openai_api_type=config[\"api_type\"],\n",
    "    temperature=0.1,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "'''\n",
    "You are a financial advisor. Answer the question based solely on the context below:\n",
    "\n",
    "<context>\n",
    "\n",
    "{context}\n",
    "\n",
    "</context>\n",
    "'''\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(vector_store.as_retriever(), question_answer_chain)\n",
    "\n",
    "result = chain.invoke({\"input\": query})\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
